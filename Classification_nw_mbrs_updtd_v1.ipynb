{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = hc.table(\"cust_exp_enc.n369087_cp_segmentation_Sub_base_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['individual_id',\n",
       " 'individual_analytics_identifier',\n",
       " 'proxy_id',\n",
       " 'funding_category',\n",
       " 'customer_sub_segment_code',\n",
       " 'medical_plan_type',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'covered_dependents_count',\n",
       " 'vision_product',\n",
       " 'dental_product',\n",
       " 'rx_ind',\n",
       " 'pas_12_months_login',\n",
       " 'action_atts',\n",
       " 'call_count',\n",
       " 'hdhp_flag',\n",
       " 'high_deductible_flag',\n",
       " 'pas_12_claims',\n",
       " 'pas_12_med_cost',\n",
       " 'pas_12_med_cost_inn',\n",
       " 'pas_12_med_cost_oon',\n",
       " 'pas_12_inn_visits',\n",
       " 'pas_12_oon_visits',\n",
       " 'pas_12_pcp_visits',\n",
       " 'urbsubr',\n",
       " 'employment_index',\n",
       " 'a_hh_median_income',\n",
       " 'higher_education_index',\n",
       " 'physical_inactivity_index',\n",
       " 'hpd_at_risk',\n",
       " 'hpd_child_chronic',\n",
       " 'hpd_behavioral_health',\n",
       " 'hpd_specialty_chronic',\n",
       " 'hpd_at_polychronic',\n",
       " 'hpd_others',\n",
       " 'hpd_category_count',\n",
       " 'disease_count',\n",
       " 'eng_ind',\n",
       " 'soe_active_not_engaged',\n",
       " 'complaint_count',\n",
       " 'appeal_count',\n",
       " 'claim_tenure',\n",
       " 'member_tenure',\n",
       " 'new_ind']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_keep = ['individual_id',\n",
    " 'individual_analytics_identifier',\n",
    " 'proxy_id',\n",
    " 'funding_category',\n",
    " 'customer_sub_segment_code',\n",
    " 'medical_plan_type',\n",
    " 'gender',\n",
    " 'age',\n",
    " 'covered_dependents_count',\n",
    " 'vision_product',\n",
    " 'dental_product',\n",
    " 'rx_ind',\n",
    " 'hdhp_flag',\n",
    " 'high_deductible_flag',\n",
    " 'urbsubr',\n",
    " 'employment_index',\n",
    " 'a_hh_median_income',\n",
    " 'higher_education_index',\n",
    " 'physical_inactivity_index',\n",
    " 'new_ind'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clssfctn = data[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(new_ind=None, t_cnt=65847, uni_cnt=65847),\n",
       " Row(new_ind=1, t_cnt=2524685, uni_cnt=2524685),\n",
       " Row(new_ind=0, t_cnt=5369927, uni_cnt=5369927)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clssfctn.groupby(\"new_ind\").agg(F.count(\"individual_analytics_identifier\").alias(\"t_cnt\"),\\\n",
    "                                    F.countDistinct(\"individual_analytics_identifier\").alias(\"uni_cnt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4_clusters\n",
    "data_clustrng = hc.table(\"cust_exp_enc.n275675_cp_segmentation_base_prd_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clssfctn_1 = data_clssfctn.join(data_clustrng.select(\"individual_analytics_identifier\", \"prediction\"),\\\n",
    "                                     \"individual_analytics_identifier\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------------+------------+----------------+-------------------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "|individual_analytics_identifier|   individual_id|    proxy_id|funding_category|customer_sub_segment_code|gender|age|covered_dependents_count|vision_product|dental_product|rx_ind|hdhp_flag|high_deductible_flag|urbsubr|employment_index|a_hh_median_income|higher_education_index|physical_inactivity_index|new_ind|prediction|\n",
      "+-------------------------------+----------------+------------+----------------+-------------------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "|                       10000108|   6887508685182|DT32G7BBBPXY|               B|                       NA|     M| 63|                       0|             0|             0|     0|        1|                   1|      S|          0.6893|           62604.0|                 0.586|                   0.4523|      0|         1|\n",
      "|                      100014461|       370818470|GT94DBBBBPXY|               B|                       NA|     M| 54|                       4|             0|             1|     0|        1|                   1|      S|          0.6965|          105634.0|                0.6167|                   0.3865|      0|         1|\n",
      "|                      100014594|1575249993044533|9VBR6BBBBPXZ|               B|                      KEY|     M| 56|                       3|             0|             0|     0|        0|                   1|      U|          0.6501|           81698.0|                0.6206|                   0.3459|      0|         1|\n",
      "|                      100018477| 943228606108216|WQT017BBBPXY|               B|                       NA|     F| 60|                       0|             0|             1|     0|        1|                   1|      S|            0.54|          106406.0|                 0.756|                   0.4666|      0|         1|\n",
      "|                      100040824|  40661813011701|D1RHQ7BBBPXY|               B|                       NA|     M| 56|                       1|             0|             0|     0|        0|                   1|      R|          0.2775|           52350.0|                0.1204|                   0.5532|      0|         1|\n",
      "|                      100051582|      3769995804|BZX0QQBBBPXY|               B|                       NA|     M| 64|                       0|             0|             0|     0|        0|                   1|      U|          0.6978|           97716.0|                0.4628|                   0.5532|      0|         1|\n",
      "|                      100112049|        89921598|2BKTVBBBBPXY|               A|                      KEY|     F| 66|                       0|             0|             0|     1|        0|                   0|      R|          0.3986|           33344.0|                0.1897|                   0.3216|      0|         1|\n",
      "|                      100119281|   9615472936823|NR349GBBBPXY|               B|                      NAG|     F| 57|                       2|             0|             1|     0|        0|                   0|      U|          0.5636|           46458.0|                0.2592|                   0.4152|      0|         1|\n",
      "|                      100140837|   9520418671667|LV1NPGBBBPXY|               B|                      NAG|     M| 56|                       1|             0|             1|     1|        0|                   0|      S|           0.461|           37318.0|                0.2803|                   0.2945|      0|         1|\n",
      "|                      100179913|   1874590461083|XF7VCGBBBPXY|               B|                       NA|     F| 54|                       1|             0|             0|     0|        0|                   0|      R|          0.6407|           86972.0|                0.4762|                   0.0054|      0|         1|\n",
      "|                      100231343|6952230491187530|0FNYV7BBBPXY|               B|                       NA|     F| 52|                       0|             0|             0|     0|        0|                   0|      R|          0.5985|           72625.0|                0.3381|                   0.3967|      0|         1|\n",
      "|                      100240252|      2302036388|7H2FVBBBBPXY|               B|                       NA|     M| 59|                       1|             0|             0|     0|        0|                   0|      R|          0.6122|           64175.0|                0.3135|                   0.2945|      0|         1|\n",
      "|                      100398424|       724121607|KJVBTBBBBPXY|               B|                       NA|     M| 59|                       0|             0|             0|     0|        1|                   1|      S|          0.4663|           34409.0|                0.2193|                   0.2945|      0|         1|\n",
      "|                      100439642|    684620763493|NHR96QBBBPXY|               B|                       NA|     M| 48|                       0|             0|             1|     0|        0|                   1|      R|          0.6372|           60833.0|                0.6648|                   0.3216|      0|         3|\n",
      "|                      100621337|        99146321|KSNBMBBBBPXY|               B|                       NA|     M| 64|                       0|             0|             0|     1|        1|                   1|      S|          0.6289|          155250.0|                0.7125|                   0.3133|      0|         1|\n",
      "|                      100621366|9377652108379477|XXR7BKBBBPXY|               B|                       NA|     M| 62|                       0|             0|             0|     1|        1|                   1|      S|          0.7373|           96926.0|                0.6285|                   0.5091|      0|         1|\n",
      "|                      100626427|         6035106|1S4RQBBBBPXY|               B|                       NA|     M| 65|                       1|             0|             0|     1|        0|                   0|      S|          0.6914|          151953.0|                0.6791|                   0.5091|      0|         2|\n",
      "|                      100627320|  33941249658326|PPYSMGBBBPXY|               B|                       NA|     M| 56|                       2|             0|             0|     1|        0|                   0|      S|          0.6721|           61295.0|                0.2414|                   0.5091|      0|         1|\n",
      "|                      100627560| 768393632382337|5D87P7BBBPXY|               B|                       NA|     M| 59|                       0|             0|             0|     1|        1|                   1|      S|          0.4395|           95020.0|                0.3371|                   0.5091|      0|         1|\n",
      "|                      100628744|        92137473|71KYHBBBBPXY|               B|                       NA|     F| 60|                       0|             0|             0|     1|        1|                   1|      S|          0.7415|           80516.0|                0.4213|                   0.5091|      0|         1|\n",
      "+-------------------------------+----------------+------------+----------------+-------------------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_clssfctn_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501368"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clssfctn_1.count()#7,828,256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_clssfctn_1.write.option(\"sep\",\"|\").format('orc').mode(\"overwrite\").saveAsTable(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_data = hc.table(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv1_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_multiple_values(df, col_list):\n",
    "    for item in col_list:\n",
    "        df = df.filter(~(F.col(item).like('%/%')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Updating urban value in urban suburban indicator :\n",
    "inp_data = inp_data.withColumn(\"urbsubr\", when(trim(col(\"urbsubr\")) == \"U\", \"Urb\").otherwise(col(\"urbsubr\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_unknown_n_nulls(df,col_list):\n",
    "    for item in col_list:\n",
    "        \n",
    "        df = df.withColumn(item, when(F.lower(F.col(item)) == 'u', None).otherwise(F.col(item)))\n",
    "        df = df.withColumn(item, when(trim(col(item)) == '',None).otherwise(col(item)))\n",
    "    df = df.na.drop()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def string_to_num(df, col_list):\n",
    "    for item in col_list:\n",
    "        categories = df.select(item).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print categories\n",
    "        exprs = [(F.when(F.col(item) == category, 1).otherwise(0)).alias(item+\"_\"+category.replace(\" \",\"\").\\\n",
    "                                                                         replace(\"(\",\"\").replace(\")\",\"\")) for category in categories]\n",
    "\n",
    "        df = df.select('*', *exprs)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_part1(data, cols_to_keep):\n",
    "    data_1 = drop_multiple_values(data, cols_to_keep)\n",
    "    #print data_1.count()\n",
    "    \n",
    "    data_2 = drop_unknown_n_nulls(data_1, cols_to_keep)\n",
    "    #print data_2.count()\n",
    "\n",
    "    \n",
    "    \n",
    "    #data_3 = string_to_num(data_2,cat_list)\n",
    "    #print data_3.count()\n",
    "    \n",
    "    return data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_list = ['funding_category',\n",
    "                 'customer_sub_segment_code',\n",
    "                 'gender',\n",
    "                 'urbsubr',\n",
    "           'medical_plan_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------+------------+----------------+-------------------------+-----------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "|individual_analytics_identifier|individual_id|    proxy_id|funding_category|customer_sub_segment_code|medical_plan_type|gender|age|covered_dependents_count|vision_product|dental_product|rx_ind|hdhp_flag|high_deductible_flag|urbsubr|employment_index|a_hh_median_income|higher_education_index|physical_inactivity_index|new_ind|prediction|\n",
      "+-------------------------------+-------------+------------+----------------+-------------------------+-----------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "|                       10002666|    171548236|XCKRGQBBBPXY|               B|                       NA|   Managed Choice|     F| 62|                       0|             0|             0|     0|        1|                   1|      S|          0.6278|           57067.0|                0.3766|                   0.4523|      0|         1|\n",
      "|                       10003911|     62761543|BVS50BBBBPXY|               B|                       NA|   Managed Choice|     F| 62|                       0|             0|             0|     0|        1|                   1|    Urb|          0.5766|           68245.0|                0.3897|                   0.4523|      0|         1|\n",
      "|                      100112738|   2457192527|FNNNVBBBBPXY|               B|                      KEY|      PPO Medical|     F| 49|                       3|             0|             1|     0|        0|                   0|    Urb|          0.7196|          136563.0|                 0.612|                   0.3884|      0|         1|\n",
      "|                      100135063|    512667167|XHD09BBBBPXY|               B|                       NA|       HMO (ACAS)|     M| 68|                       1|             0|             0|     0|        0|                   0|    Urb|          0.5995|           49688.0|                0.1765|                   0.4152|      0|         1|\n",
      "|                      100219906|  45491504550|KKV8MQBBBPXY|               B|                       NA|   Managed Choice|     F| 65|                       1|             0|             0|     0|        0|                   0|      R|          0.4798|           40993.0|                0.2703|                   0.3216|      0|         1|\n",
      "+-------------------------------+-------------+------------+----------------+-------------------------+-----------------+------+---+------------------------+--------------+--------------+------+---------+--------------------+-------+----------------+------------------+----------------------+-------------------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pre_processed = preprocess_part1(inp_data,cols_to_keep )\n",
    "data_pre_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pre_processed.write.option(\"sep\",\"|\").format('orc').mode(\"overwrite\").saveAsTable(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv2_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_pre_process = hc.table(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv2_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_scale_features = ['age',\n",
    "             'covered_dependents_count',\n",
    "                     'a_hh_median_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_scaled_pre_process = post_pre_process\n",
    "for item in to_scale_features:\n",
    "    vec_assembler = VectorAssembler(inputCols= [item] , outputCol = \"vec\"+item)\n",
    "    scaler = StandardScaler(inputCol=\"vec\"+item, outputCol= 'scaled_'+item)\n",
    "    vec_scaler_pipeline = Pipeline(stages=[vec_assembler, scaler])\n",
    "    scaled = vec_scaler_pipeline.fit(_scaled_pre_process)\n",
    "    _scaled_pre_process = scaled.transform(_scaled_pre_process)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_scaled = _scaled_pre_process.drop('age',\n",
    "                                       'covered_dependents_count',\n",
    "                                       'a_hh_median_income',\n",
    "                                       'vecage',\n",
    "                                       'veccovered_dependents_count',\n",
    "                                       'veca_hh_median_income'\n",
    "                                      )\n",
    "for item in to_scale_features:\n",
    "    \n",
    "    data_scaled = data_scaled.withColumnRenamed(\"scaled_\"+item, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['funding_category',\n",
       " 'customer_sub_segment_code',\n",
       " 'gender',\n",
       " 'urbsubr',\n",
       " 'medical_plan_type']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_list = ['vision_product',\n",
    "            'dental_product',\n",
    "            'rx_ind',\n",
    "            'hdhp_flag',\n",
    "            'high_deductible_flag',\n",
    "            'employment_index',\n",
    "            'higher_education_index',\n",
    "            'physical_inactivity_index',\n",
    "            'new_ind',\n",
    "            'age',\n",
    "            'covered_dependents_count',\n",
    "            'a_hh_median_income'\n",
    "           ]\n",
    "labelcol = 'prediction'\n",
    "indexcol = 'individual_analytics_identifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical variables & creating features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['individual_analytics_identifier',\n",
       " 'individual_id',\n",
       " 'proxy_id',\n",
       " 'funding_category',\n",
       " 'customer_sub_segment_code',\n",
       " 'medical_plan_type',\n",
       " 'gender',\n",
       " 'vision_product',\n",
       " 'dental_product',\n",
       " 'rx_ind',\n",
       " 'hdhp_flag',\n",
       " 'high_deductible_flag',\n",
       " 'urbsubr',\n",
       " 'employment_index',\n",
       " 'higher_education_index',\n",
       " 'physical_inactivity_index',\n",
       " 'new_ind',\n",
       " 'prediction',\n",
       " 'age',\n",
       " 'covered_dependents_count',\n",
       " 'a_hh_median_income',\n",
       " 'funding_category_indexed',\n",
       " 'customer_sub_segment_code_indexed',\n",
       " 'gender_indexed',\n",
       " 'urbsubr_indexed',\n",
       " 'medical_plan_type_indexed',\n",
       " 'funding_category_indexed_encoded',\n",
       " 'customer_sub_segment_code_indexed_encoded',\n",
       " 'gender_indexed_encoded',\n",
       " 'urbsubr_indexed_encoded',\n",
       " 'medical_plan_type_indexed_encoded',\n",
       " 'features',\n",
       " 'label']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = get_dummy(data_scaled,indexcol,cat_list,con_list,labelcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ref Col:\n",
    "funding_category: C\n",
    "customer_sub_segment_code: BOA\n",
    "medical_plan_type: Indemnity Medical\n",
    "gender: F\n",
    "urbsubr: S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------+\n",
      "|funding_category|funding_category_indexed_encoded|\n",
      "+----------------+--------------------------------+\n",
      "|               C|                       (2,[],[])|\n",
      "|               B|                   (2,[0],[1.0])|\n",
      "|               A|                   (2,[1],[1.0])|\n",
      "+----------------+--------------------------------+\n",
      "\n",
      "None\n",
      "+-------------------------+-----------------------------------------+\n",
      "|customer_sub_segment_code|customer_sub_segment_code_indexed_encoded|\n",
      "+-------------------------+-----------------------------------------+\n",
      "|                       NA|                            (6,[0],[1.0])|\n",
      "|                      SEL|                            (6,[3],[1.0])|\n",
      "|                      NAG|                            (6,[2],[1.0])|\n",
      "|                      FED|                            (6,[5],[1.0])|\n",
      "|                       SG|                            (6,[4],[1.0])|\n",
      "|                      KEY|                            (6,[1],[1.0])|\n",
      "|                      BOA|                                (6,[],[])|\n",
      "+-------------------------+-----------------------------------------+\n",
      "\n",
      "None\n",
      "+-----------------+---------------------------------+\n",
      "|medical_plan_type|medical_plan_type_indexed_encoded|\n",
      "+-----------------+---------------------------------+\n",
      "|      QPOS (ACAS)|                    (4,[3],[1.0])|\n",
      "|      PPO Medical|                    (4,[1],[1.0])|\n",
      "|       HMO (ACAS)|                    (4,[2],[1.0])|\n",
      "|Indemnity Medical|                        (4,[],[])|\n",
      "|   Managed Choice|                    (4,[0],[1.0])|\n",
      "+-----------------+---------------------------------+\n",
      "\n",
      "None\n",
      "+------+----------------------+\n",
      "|gender|gender_indexed_encoded|\n",
      "+------+----------------------+\n",
      "|     F|             (1,[],[])|\n",
      "|     M|         (1,[0],[1.0])|\n",
      "+------+----------------------+\n",
      "\n",
      "None\n",
      "+-------+-----------------------+\n",
      "|urbsubr|urbsubr_indexed_encoded|\n",
      "+-------+-----------------------+\n",
      "|      S|              (2,[],[])|\n",
      "|      R|          (2,[1],[1.0])|\n",
      "|    Urb|          (2,[0],[1.0])|\n",
      "+-------+-----------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print model_input.select('funding_category','funding_category_indexed_encoded').distinct().show()\n",
    "print model_input.select('customer_sub_segment_code', 'customer_sub_segment_code_indexed_encoded').distinct().show()\n",
    "print model_input.select('medical_plan_type','medical_plan_type_indexed_encoded').distinct().show()\n",
    "print model_input.select('gender','gender_indexed_encoded').distinct().show()\n",
    "print model_input.select('urbsubr','urbsubr_indexed_encoded').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input_1 = model_input.select(\"individual_analytics_identifier\",\"new_ind\",\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input_1.write.option(\"sep\",\"|\").format('orc').mode(\"overwrite\").saveAsTable(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv3_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_inp = hc.table(\"cust_exp_enc.n201366_segmentation_2020_nw_mbrs_clssfctn_inpv3_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 5442085\n",
      "Test Dataset Count: 2334584\n"
     ]
    }
   ],
   "source": [
    "train, test = model_inp.randomSplit([0.7, 0.3], seed = 50)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictions  = lrModel.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions  = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786088428251\n",
      "0.786074210609\n"
     ]
    }
   ],
   "source": [
    "print evaluator.evaluate(train_predictions)\n",
    "print evaluator.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+--------------------+--------------------+----------+\n",
      "|individual_analytics_identifier|label|       rawPrediction|         probability|prediction|\n",
      "+-------------------------------+-----+--------------------+--------------------+----------+\n",
      "|                      100001429|    1|[-3.8815428363057...|[9.83973344110295...|       1.0|\n",
      "|                      100005390|    1|[-2.6808948165222...|[0.00678951406410...|       3.0|\n",
      "|                      100006271|    3|[-1.3718995945003...|[0.03373148351050...|       3.0|\n",
      "|                      100007383|    2|[-2.0227824327116...|[1.43299474474640...|       2.0|\n",
      "|                      100007493|    1|[-4.1549283660421...|[3.37475693096345...|       1.0|\n",
      "|                      100008521|    2|[-4.1383062673373...|[3.71105075769068...|       1.0|\n",
      "|                      100008817|    1|[-5.6483357814647...|[2.73167733178818...|       1.0|\n",
      "|                      100009378|    1|[-4.8874226844012...|[8.80139760089523...|       1.0|\n",
      "|                       10000944|    1|[-6.2419514756094...|[5.01243067572091...|       1.0|\n",
      "|                      100011749|    3|[-2.2438205349814...|[0.00849714175593...|       3.0|\n",
      "+-------------------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions.select('individual_analytics_identifier', 'label', 'rawPrediction','probability', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_crschck = train_predictions.groupby(\"new_ind\",\"label\",\"prediction\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_ind</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>344146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1318428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>237553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>630192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>153542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1250235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    new_ind  label  prediction    count\n",
       "9         0      0         3.0   344146\n",
       "3         0      0         0.0    53618\n",
       "17        1      0         2.0    54485\n",
       "6         0      0         1.0     7070\n",
       "7         1      0         0.0   990131\n",
       "10        0      0         2.0     5579\n",
       "1         0      1         3.0    80317\n",
       "2         0      1         2.0    53150\n",
       "8         0      1         1.0  1318428\n",
       "16        0      2         0.0       51\n",
       "14        0      2         2.0    44881\n",
       "13        1      2         1.0      317\n",
       "0         0      2         3.0    66089\n",
       "11        0      2         1.0   237553\n",
       "18        1      2         0.0    67714\n",
       "5         1      2         2.0   630192\n",
       "15        0      3         0.0    55924\n",
       "4         0      3         1.0   153542\n",
       "12        0      3         3.0  1250235\n",
       "19        0      3         2.0    28583"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_crschck.sort_values(by = [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_ind</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>147234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>424018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>565335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>269997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>535628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    new_ind  label  prediction   count\n",
       "9         0      0         3.0  147234\n",
       "3         0      0         0.0   23061\n",
       "17        1      0         2.0   23640\n",
       "6         0      0         1.0    3048\n",
       "7         1      0         0.0  424018\n",
       "10        0      0         2.0    2424\n",
       "1         0      1         3.0   34218\n",
       "2         0      1         2.0   22689\n",
       "8         0      1         1.0  565335\n",
       "16        0      2         0.0      25\n",
       "14        0      2         2.0   19297\n",
       "13        1      2         1.0     102\n",
       "0         0      2         3.0   28675\n",
       "11        0      2         1.0  101936\n",
       "18        1      2         0.0   29034\n",
       "5         1      2         2.0  269997\n",
       "15        0      3         0.0   24043\n",
       "4         0      3         1.0   66121\n",
       "12        0      3         3.0  535628\n",
       "19        0      3         2.0   12410"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_crschck = test_predictions.groupby(\"new_ind\",\"label\",\"prediction\").count().toPandas()\n",
    "test_crschck.sort_values(by = [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "DenseMatrix([[  2.18154165e-01,   4.78341096e-01,   3.54696944e-01,\n",
      "                4.35297430e-01,  -2.76210205e-01,   4.26787254e-01,\n",
      "                7.22021903e-01,  -1.54116208e+00,   1.75597338e-01,\n",
      "                3.92843033e-01,   1.72763812e-01,   7.81739469e-01,\n",
      "                2.88887594e-01,   4.92271448e-02,  -1.69325705e-02,\n",
      "                2.51011791e-02,  -1.20700367e-01,   3.56610731e-01,\n",
      "               -8.39744541e-03,  -2.81018630e-01,   7.82509704e+00,\n",
      "                1.95134563e-01,   1.64100117e+00,   4.54362456e+00,\n",
      "               -2.44962861e+00,  -2.69670485e-01,  -1.93400421e-01],\n",
      "             [ -1.59282511e-01,  -6.66521828e-01,  -2.67269120e-01,\n",
      "               -4.66936068e-01,  -2.20647325e-01,  -5.13079812e-01,\n",
      "               -8.24317904e-01,   2.47141066e+00,  -1.44076299e-01,\n",
      "               -4.67901894e-01,  -2.66653095e-01,  -8.85856837e-01,\n",
      "               -3.74510785e-01,  -1.62018323e-01,   7.54762661e-04,\n",
      "               -2.03470792e-02,   1.58023269e-01,  -4.20473615e-01,\n",
      "                5.99064769e-02,   2.38990133e-01,  -9.14112143e+00,\n",
      "               -2.07133413e-01,  -1.93005596e+00,  -4.03020584e+00,\n",
      "                2.50561759e+00,  -8.78340137e-03,   2.19150668e-01],\n",
      "             [ -4.19042069e-01,   4.23691112e-02,  -2.00758021e-01,\n",
      "               -6.56721414e-02,   7.00945190e-02,   1.85581354e-02,\n",
      "                3.57528554e-01,  -8.41679162e-01,  -1.99761305e-01,\n",
      "               -2.21098166e-01,  -7.00747862e-02,  -1.02717018e-01,\n",
      "               -2.03871623e-01,  -4.48259376e-01,  -6.31082756e-01,\n",
      "                1.14797459e-02,  -4.25848401e-01,  -1.62071897e-01,\n",
      "               -1.46793783e-01,  -9.15839490e-02,  -4.34149043e+00,\n",
      "               -1.95360692e-01,  -1.60612432e+00,   3.52389822e+00,\n",
      "                1.45479155e+00,  -5.93414282e-02,  -2.99960225e-03],\n",
      "             [  3.60170416e-01,   1.45811621e-01,   1.13330198e-01,\n",
      "                9.73107799e-02,   4.26763011e-01,   6.77344226e-02,\n",
      "               -2.55232552e-01,  -8.85694213e-02,   1.68240266e-01,\n",
      "                2.96157027e-01,   1.63964070e-01,   2.06834386e-01,\n",
      "                2.89494814e-01,   5.61050554e-01,   6.47260564e-01,\n",
      "               -1.62338459e-02,   3.88525499e-01,   2.25934780e-01,\n",
      "                9.52847512e-02,   1.33612446e-01,   5.65751481e+00,\n",
      "                2.07359542e-01,   1.89517911e+00,  -4.03731694e+00,\n",
      "               -1.51078053e+00,   3.37795314e-01,  -2.27506443e-02]])\n"
     ]
    }
   ],
   "source": [
    "print \"Coefficients: \\n\" + str(lrModel.coefficientMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2104.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 242.0 failed 4 times, most recent failure: Lost task 0.3 in stage 242.0 (TID 16050, xhadoopm1379p.aetna.com, executor 1389): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 26.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:106)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:104)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:98)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:374)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1203)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredFeatures(ChiSqTest.scala:120)\n\tat org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:176)\n\tat org.apache.spark.mllib.feature.ChiSqSelector.fit(ChiSqSelector.scala:235)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:170)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 26.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:106)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:104)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:98)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-837b64867dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Aspect'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aspect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2104.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 242.0 failed 4 times, most recent failure: Lost task 0.3 in stage 242.0 (TID 16050, xhadoopm1379p.aetna.com, executor 1389): org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 26.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:106)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:104)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:98)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:375)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:374)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1204)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.countByValue(RDD.scala:1203)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredFeatures(ChiSqTest.scala:120)\n\tat org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:176)\n\tat org.apache.spark.mllib.feature.ChiSqSelector.fit(ChiSqSelector.scala:235)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:170)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Chi-square test expect factors (categorical values) but found more than 10000 distinct values in column 26.\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:106)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1$$anonfun$apply$2.apply(ChiSqTest.scala:104)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:104)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$3$$anonfun$apply$1.apply(ChiSqTest.scala:98)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using chisquareSelector\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "css = ChiSqSelector(featuresCol='features',outputCol='Aspect',labelCol='label',fpr=0.05)\n",
    "train=css.fit(train).transform(train)\n",
    "test=css.fit(test).transform(test)\n",
    "test.select(\"Aspect\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------+--------------------+-----+\n",
      "|individual_analytics_identifier|new_ind|            features|label|\n",
      "+-------------------------------+-------+--------------------+-----+\n",
      "|                       10000031|      0|(27,[0,2,8,10,11,...|    1|\n",
      "|                      100000531|      0|(27,[0,4,8,12,17,...|    3|\n",
      "|                      100002342|      0|(27,[0,4,9,11,15,...|    3|\n",
      "|                      100002947|      0|(27,[0,2,9,11,16,...|    3|\n",
      "|                       10000328|      0|(27,[0,2,10,11,18...|    2|\n",
      "|                      100003456|      1|(27,[0,2,8,11,20,...|    2|\n",
      "|                       10000368|      0|(27,[0,2,8,11,20,...|    1|\n",
      "|                      100003915|      0|(27,[0,4,11,15,16...|    1|\n",
      "|                      100004198|      0|(27,[0,2,8,9,11,2...|    1|\n",
      "|                      100004287|      0|(27,[0,2,8,10,11,...|    1|\n",
      "|                      100004528|      0|(27,[0,3,9,11,16,...|    3|\n",
      "|                      100005128|      0|(27,[0,4,11,15,16...|    1|\n",
      "|                      100005261|      0|(27,[0,2,10,11,20...|    1|\n",
      "|                      100005435|      0|(27,[0,4,9,11,15,...|    1|\n",
      "|                      100005451|      0|(27,[0,3,10,11,16...|    1|\n",
      "|                       10000567|      0|(27,[0,2,11,18,19...|    1|\n",
      "|                      100006271|      0|(27,[0,4,10,11,15...|    3|\n",
      "|                      100006509|      0|(27,[1,6,8,9,13,1...|    3|\n",
      "|                      100007383|      1|(27,[0,2,9,11,19,...|    2|\n",
      "|                      100007493|      0|(27,[0,2,11,20,21...|    1|\n",
      "+-------------------------------+-------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection: Trial Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance_list = pd.Series(lrModel.coefficientMatrix.toArray()[0])\n",
    "sorted_imp = importance_list.sort_values(ascending= False)\n",
    "kept = list((sorted_imp[sorted_imp > 0.03]).index)\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "vector_slicer = VectorSlicer(inputCol= \"features\", indices= kept, outputCol= \"feature_subset\")\n",
    "with_selected_feature = vector_slicer.transform(model_input_1)\n",
    "with_selected_feature.show(truncate=False)\n",
    "lr_subset = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel_ss = lr_subset.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    7.825097\n",
       "23    4.543625\n",
       "22    1.641001\n",
       "11    0.781739\n",
       "6     0.722022\n",
       "1     0.478341\n",
       "3     0.435297\n",
       "5     0.426787\n",
       "9     0.392843\n",
       "17    0.356611\n",
       "2     0.354697\n",
       "12    0.288888\n",
       "0     0.218154\n",
       "21    0.195135\n",
       "8     0.175597\n",
       "10    0.172764\n",
       "13    0.049227\n",
       "15    0.025101\n",
       "18   -0.008397\n",
       "14   -0.016933\n",
       "16   -0.120700\n",
       "26   -0.193400\n",
       "25   -0.269670\n",
       "4    -0.276210\n",
       "19   -0.281019\n",
       "7    -1.541162\n",
       "24   -2.449629\n",
       "dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_subset = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel_ss = lr_subset.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictions_ss  = lrModel_ss.transform(train)\n",
    "test_predictions_ss  = lrModel_ss.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator as MCE\n",
    "evaluator = MCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785831900805\n",
      "0.786046466313\n"
     ]
    }
   ],
   "source": [
    "print evaluator.evaluate(train_predictions_ss)\n",
    "print evaluator.evaluate(test_predictions_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation & Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)\n",
    "prediction = cvModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7862055650361585"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.62585346e-01,   2.88983287e-01,   2.30580164e-01,\n",
       "          2.66366670e-01,  -1.70252750e-01,   2.54847897e-01,\n",
       "          4.78552552e-01,  -9.39517838e-01,   1.15811930e-01,\n",
       "          2.48982489e-01,   1.20944902e-01,   5.17602574e-01,\n",
       "          1.92771501e-01,   2.95027657e-03,  -2.86615665e-02,\n",
       "         -1.75722700e-03,  -9.26429658e-02,   2.14650450e-01,\n",
       "         -6.74615368e-03,  -1.86233051e-01,   5.04151616e+00,\n",
       "          1.10973690e-01,   1.08707564e+00,   2.90429446e+00,\n",
       "         -1.60106183e+00,  -1.85791396e-01,  -1.30685571e-01],\n",
       "       [ -9.87368525e-02,  -4.33569652e-01,  -1.43607660e-01,\n",
       "         -2.94003891e-01,  -1.55729287e-01,  -3.32062630e-01,\n",
       "         -5.76566601e-01,   1.49830750e+00,  -8.79471807e-02,\n",
       "         -2.86562136e-01,  -1.93920249e-01,  -5.78865253e-01,\n",
       "         -2.48378896e-01,  -7.75651968e-02,   2.84576639e-02,\n",
       "          7.90626139e-03,   1.25433466e-01,  -2.65288922e-01,\n",
       "          5.61108164e-02,   1.70225166e-01,  -5.82603214e+00,\n",
       "         -7.54857725e-02,  -1.26480675e+00,  -2.61296784e+00,\n",
       "          1.63751352e+00,  -1.35168509e-02,   1.52948839e-01],\n",
       "       [ -2.97246066e-01,   3.04173616e-02,  -1.49877011e-01,\n",
       "         -4.35349403e-02,   1.49257295e-02,   1.64254683e-02,\n",
       "          2.96286832e-01,  -5.32323700e-01,  -1.44868567e-01,\n",
       "         -1.53806681e-01,  -4.85250336e-02,  -4.79839186e-02,\n",
       "         -1.65690642e-01,  -3.42979181e-01,  -4.81155897e-01,\n",
       "         -1.04608204e-02,  -3.07349490e-01,  -1.09613224e-01,\n",
       "         -1.04787776e-01,  -6.97117279e-02,  -3.11088035e+00,\n",
       "         -1.67520925e-01,  -1.15122553e+00,   2.32380230e+00,\n",
       "          9.67430341e-01,  -3.51460822e-02,  -8.20392458e-03],\n",
       "       [  2.33397572e-01,   1.14169004e-01,   6.29045080e-02,\n",
       "          7.11721617e-02,   3.11056307e-01,   6.07892642e-02,\n",
       "         -1.98272783e-01,  -2.64659606e-02,   1.17003818e-01,\n",
       "          1.91386329e-01,   1.21500381e-01,   1.09246598e-01,\n",
       "          2.21298037e-01,   4.17594101e-01,   4.81359799e-01,\n",
       "          4.31178597e-03,   2.74558990e-01,   1.60251696e-01,\n",
       "          5.54231130e-02,   8.57196128e-02,   3.89539633e+00,\n",
       "          1.32033007e-01,   1.32895663e+00,  -2.61512892e+00,\n",
       "         -1.00388203e+00,   2.34454329e-01,  -1.40593438e-02]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.01\n",
      "Best Param (ElasticNetParam):  0.0\n",
      "Best Param (MaxIter):  10\n"
     ]
    }
   ],
   "source": [
    "print 'Best Param (regParam): ', bestModel._java_obj.getRegParam()\n",
    "print 'Best Param (ElasticNetParam): ', bestModel._java_obj.getElasticNetParam()\n",
    "print 'Best Param (MaxIter): ', bestModel._java_obj.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[individual_analytics_identifier: string, new_ind: int, features: vector, label: int, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge = model_input.join(train_predictions, \"individual_analytics_identifier\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------------+------------+----------------+-------------------------+-----------------+------+--------------+--------------+------+---------+--------------------+-------+----------------+----------------------+-------------------------+-------+----------+--------------------+------------------------+--------------------+------------------------+---------------------------------+--------------+---------------+-------------------------+--------------------------------+-----------------------------------------+----------------------+-----------------------+---------------------------------+--------------------+-----+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|individual_analytics_identifier|   individual_id|    proxy_id|funding_category|customer_sub_segment_code|medical_plan_type|gender|vision_product|dental_product|rx_ind|hdhp_flag|high_deductible_flag|urbsubr|employment_index|higher_education_index|physical_inactivity_index|new_ind|prediction|                 age|covered_dependents_count|  a_hh_median_income|funding_category_indexed|customer_sub_segment_code_indexed|gender_indexed|urbsubr_indexed|medical_plan_type_indexed|funding_category_indexed_encoded|customer_sub_segment_code_indexed_encoded|gender_indexed_encoded|urbsubr_indexed_encoded|medical_plan_type_indexed_encoded|            features|label|new_ind|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-------------------------------+----------------+------------+----------------+-------------------------+-----------------+------+--------------+--------------+------+---------+--------------------+-------+----------------+----------------------+-------------------------+-------+----------+--------------------+------------------------+--------------------+------------------------+---------------------------------+--------------+---------------+-------------------------+--------------------------------+-----------------------------------------+----------------------+-----------------------+---------------------------------+--------------------+-----+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|                       10000108|   6887508685182|DT32G7BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     0|        1|                   1|      S|          0.6893|                 0.586|                   0.4523|      0|         1| [4.570880414622636]|                   [0.0]| [2.013115016617298]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,18,...|    1|      0|(27,[0,2,8,11,18,...|    1|[-3.8982861498258...|[6.29281397464539...|       1.0|\n",
      "|                      100014461|       370818470|GT94DBBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             1|     0|        1|                   1|      S|          0.6965|                0.6167|                   0.3865|      0|         1|[3.9178974982479735]|     [3.037696867957454]|[3.3968019881373666]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,16,...|    1|      0|(27,[0,2,8,11,16,...|    1|[-3.5551518226050...|[0.00200414087168...|       1.0|\n",
      "|                      100014594|1575249993044533|9VBR6BBBBPXZ|               B|                      KEY|   Managed Choice|     M|             0|             0|     0|        0|                   1|    Urb|          0.6501|                0.6206|                   0.3459|      0|         1| [4.063004812997899]|    [2.2782726509680904]| [2.627108022292506]|                     0.0|                              1.0|           0.0|            0.0|                      0.0|                   (2,[0],[1.0])|                            (6,[1],[1.0])|         (1,[0],[1.0])|          (2,[0],[1.0])|                    (4,[0],[1.0])|(27,[0,3,8,9,11,1...|    1|      0|(27,[0,3,8,9,11,1...|    1|[-3.3772709370845...|[0.00297330205967...|       1.0|\n",
      "|                      100018477| 943228606108216|WQT017BBBPXY|               B|                       NA|   Managed Choice|     F|             0|             1|     0|        1|                   1|      S|            0.54|                 0.756|                   0.4666|      0|         1| [4.353219442497748]|                   [0.0]|[3.4216266765411194]|                     0.0|                              0.0|           1.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|             (1,[],[])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,11,16,18...|    1|      0|(27,[0,2,11,16,18...|    1|[-5.0386687118470...|[5.72396315777923...|       1.0|\n",
      "|                      100040824|  40661813011701|D1RHQ7BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     0|        0|                   1|      R|          0.2775|                0.1204|                   0.5532|      0|         1| [4.063004812997899]|    [0.7594242169893635]|[1.6833839869643403]|                     0.0|                              0.0|           0.0|            1.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[0,2,8,10,11,...|    1|      0|(27,[0,2,8,10,11,...|    1|[-5.7630396503521...|[1.43872364332745...|       1.0|\n",
      "|                      100051582|      3769995804|BZX0QQBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     0|        0|                   1|    Urb|          0.6978|                0.4628|                   0.5532|      0|         1| [4.643434071997598]|                   [0.0]| [3.142188150338252]|                     0.0|                              0.0|           0.0|            0.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|          (2,[0],[1.0])|                    (4,[0],[1.0])|(27,[0,2,8,9,11,1...|    1|      0|(27,[0,2,8,9,11,1...|    1|[-3.6873990535544...|[0.00105277204980...|       1.0|\n",
      "|                      100112049|        89921598|2BKTVBBBBPXY|               A|                      KEY|   Managed Choice|     F|             0|             0|     1|        0|                   0|      R|          0.3986|                0.1897|                   0.3216|      0|         1| [4.788541386747523]|                   [0.0]|[1.0722207385165037]|                     1.0|                              1.0|           1.0|            1.0|                      0.0|                   (2,[1],[1.0])|                            (6,[1],[1.0])|             (1,[],[])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[1,3,10,11,17...|    1|      0|(27,[1,3,10,11,17...|    1|[-5.8200373195768...|[1.33478921436777...|       1.0|\n",
      "|                      100140837|   9520418671667|LV1NPGBBBPXY|               B|                      NAG|   Managed Choice|     M|             0|             1|     1|        0|                   0|      S|           0.461|                0.2803|                   0.2945|      0|         1| [4.063004812997899]|    [0.7594242169893635]|[1.2000100023979992]|                     0.0|                              2.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[2],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,4,8,11,16,...|    1|      0|(27,[0,4,8,11,16,...|    1|[-4.9120657285921...|[1.40718610177702...|       1.0|\n",
      "|                      100231343|6952230491187530|0FNYV7BBBPXY|               B|                       NA|   Managed Choice|     F|             0|             0|     0|        0|                   0|      R|          0.5985|                0.3381|                   0.3967|      0|         1|[3.7727901834980484]|                   [0.0]|[2.3353536208841494]|                     0.0|                              0.0|           1.0|            1.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|             (1,[],[])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[0,2,10,11,20...|    1|      0|(27,[0,2,10,11,20...|    1|[-2.5692991138277...|[0.00704820103632...|       1.0|\n",
      "|                      100240252|      2302036388|7H2FVBBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     0|        0|                   0|      R|          0.6122|                0.3135|                   0.2945|      0|         1| [4.280665785122785]|    [0.7594242169893635]| [2.063632614392293]|                     0.0|                              0.0|           0.0|            1.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[0,2,8,10,11,...|    1|      0|(27,[0,2,8,10,11,...|    1|[-3.8570720066751...|[7.44441737442211...|       1.0|\n",
      "|                      100439642|    684620763493|NHR96QBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             1|     0|        0|                   1|      R|          0.6372|                0.6648|                   0.3216|      0|         3|[3.4825755539981986]|                   [0.0]|[1.9561661524164606]|                     0.0|                              0.0|           0.0|            1.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[0,2,8,10,11,...|    3|      0|(27,[0,2,8,10,11,...|    3|[-1.7737343601320...|[0.02671541130658...|       1.0|\n",
      "|                      100621337|        99146321|KSNBMBBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        1|                   1|      S|          0.6289|                0.7125|                   0.3133|      0|         1| [4.643434071997598]|                   [0.0]| [4.992270563060436]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-4.9656116255387...|[7.08625369822708...|       1.0|\n",
      "|                      100626427|         6035106|1S4RQBBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        0|                   0|      S|          0.6914|                0.6791|                   0.5091|      0|         2| [4.715987729372561]|    [0.7594242169893635]| [4.886251136030419]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    2|      0|(27,[0,2,8,11,17,...|    2|[-4.2371249179064...|[4.00961689614646...|       1.0|\n",
      "|                      100627320|  33941249658326|PPYSMGBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        0|                   0|      S|          0.6721|                0.2414|                   0.5091|      0|         1| [4.063004812997899]|     [1.518848433978727]|[1.9710223778601574]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-2.5224544266453...|[0.01034240676086...|       1.0|\n",
      "|                      100627560| 768393632382337|5D87P7BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        1|                   1|      S|          0.4395|                0.3371|                   0.5091|      0|         1| [4.280665785122785]|                   [0.0]|[3.0554946789178916]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-4.9386878141849...|[6.54017794302892...|       1.0|\n",
      "|                      100628744|        92137473|71KYHBBBBPXY|               B|                       NA|   Managed Choice|     F|             0|             0|     1|        1|                   1|      S|          0.7415|                0.4213|                   0.5091|      0|         1| [4.353219442497748]|                   [0.0]| [2.589099237715775]|                     0.0|                              0.0|           1.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|             (1,[],[])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,11,17,18...|    1|      0|(27,[0,2,11,17,18...|    1|[-2.8268130953746...|[0.00528045063338...|       1.0|\n",
      "|                      100638354| 495207722063738|04DLZ7BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        0|                   0|      S|          0.7415|                0.4213|                   0.5091|      0|         1| [5.078756016247373]|    [0.7594242169893635]| [2.589099237715775]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-4.3440381294550...|[3.40349399099710...|       1.0|\n",
      "|                      100640100|  88164640631603|LZKPXGBBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        1|                   1|      S|          0.7527|                0.4836|                   0.5532|      0|         1| [4.643434071997598]|    [0.7594242169893635]|[2.9939153028904473]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-3.4751948903545...|[0.00191813421559...|       1.0|\n",
      "|                      100644680| 421901244400954|8JQ347BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        0|                   0|      S|          0.4395|                0.3371|                   0.5091|      0|         1| [4.280665785122785]|     [3.037696867957454]|[3.0554946789178916]|                     0.0|                              0.0|           0.0|            2.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|              (2,[],[])|                    (4,[0],[1.0])|(27,[0,2,8,11,17,...|    1|      0|(27,[0,2,8,11,17,...|    1|[-5.4696172943919...|[5.15103265042493...|       1.0|\n",
      "|                      100645544| 581464440366935|RLKS67BBBPXY|               B|                       NA|   Managed Choice|     M|             0|             0|     1|        0|                   0|      R|          0.4395|                0.3371|                   0.5091|      0|         1| [4.208112127747824]|    [0.7594242169893635]|[3.0554946789178916]|                     0.0|                              0.0|           0.0|            1.0|                      0.0|                   (2,[0],[1.0])|                            (6,[0],[1.0])|         (1,[0],[1.0])|          (2,[1],[1.0])|                    (4,[0],[1.0])|(27,[0,2,8,10,11,...|    1|      0|(27,[0,2,8,10,11,...|    1|[-4.5061135328596...|[1.99852643525558...|       1.0|\n",
      "+-------------------------------+----------------+------------+----------------+-------------------------+-----------------+------+--------------+--------------+------+---------+--------------------+-------+----------------+----------------------+-------------------------+-------+----------+--------------------+------------------------+--------------------+------------------------+---------------------------------+--------------+---------------+-------------------------+--------------------------------+-----------------------------------------+----------------------+-----------------------+---------------------------------+--------------------+-----+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_merge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi Square Tests for Binary, discrete and nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_list_wo_income = [x for x in con_list if x != 'a_hh_median_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_num(df, col_list):\n",
    "    for item in col_list:\n",
    "        categories = df.select(item).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "        print categories\n",
    "        exprs = [(F.when(F.col(item) == categories[i], 1).otherwise(0)).\\\n",
    "                 alias(item+\"_\"+categories[i].replace(\" \",\"\").\\\n",
    "                 replace(\"(\",\"\").replace(\")\",\"\")) for i in range(0, len(categories)-1)]\n",
    "\n",
    "        df = df.select('*', *exprs)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1502.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3067.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3067.0 (TID 35311, xhadoopm775p.aetna.com, executor 1447): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 110829757 for n201366) is expired\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)\n\tat org.apache.spark.deploy.SparkHadoopUtil.listLeafStatuses(SparkHadoopUtil.scala:201)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.listOrcFiles(OrcFileOperator.scala:94)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:67)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:162)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:156)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor214.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 110829757 for n201366) is expired\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)\n\tat org.apache.spark.deploy.SparkHadoopUtil.listLeafStatuses(SparkHadoopUtil.scala:201)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.listOrcFiles(OrcFileOperator.scala:94)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:67)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:162)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:156)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-e39b2af60f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1502.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3067.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3067.0 (TID 35311, xhadoopm775p.aetna.com, executor 1447): org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 110829757 for n201366) is expired\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)\n\tat org.apache.spark.deploy.SparkHadoopUtil.listLeafStatuses(SparkHadoopUtil.scala:201)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.listOrcFiles(OrcFileOperator.scala:94)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:67)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:162)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:156)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor214.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 110829757 for n201366) is expired\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)\n\tat org.apache.spark.deploy.SparkHadoopUtil.listLeafStatuses(SparkHadoopUtil.scala:201)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.listOrcFiles(OrcFileOperator.scala:94)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:67)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:77)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:162)\n\tat org.apache.spark.sql.hive.orc.OrcFileFormat$$anonfun$buildReader$2.apply(OrcFileFormat.scala:156)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:138)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:122)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:168)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data_scaled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input = get_dummy(data_scaled,indexcol,cat_list,con_list,labelcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
